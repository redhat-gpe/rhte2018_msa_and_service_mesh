:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Circuit Breaker Lab

== Lab Overview

This exercise shows how to inject faults and test the resiliency of your application. Istio provides a set of failure recovery features that can be taken advantage of by the services in an application. Features include:

* Timeouts
* Bounded retries with timeout budgets and variable jitter between retries
* Limits on number of concurrent connections and requests to upstream services
* Active (periodic) health checks on each member of the load balancing pool
* Fine-grained circuit breakers (passive health checks) – applied per instance in the load balancing pool

Together, these features enable the service mesh to tolerate failing nodes and prevent localized failures from cascading instability to other nodes.

== Goals

In this lab, you will learn how to:

* Implement the pool ejection resilience strategy to increase the overall availability
* Leverage the circuit breaker pattern to avoid multiple concurrent requests to an instance

[NOTE]
._Before Start_
====
You should have NO virtualservice nor destinationrule. Run this command to clean up:

----
cd ~/lab/rhte-msa-and-service-mesh/

scripts/clean.sh $OCP_TUTORIAL_PROJECT
----
====

== Update Application Code

In this section, you will update the CatalogVerticle to expose additional endpoints for testing

. Edit the file the CatalogVerticle:
+
----
cd ~/lab/rhte-msa-and-service-mesh/catalog/java/vertx

vi src/main/java/com/redhat/developer/demos/catalog/CatalogVerticle.java
----

. Uncomment the line `router.get("/").handler(this::timeout);`

* This modification will add a three second delay for requests for the `catalog` service `v2`. 
. Save the file and exit vi.

. Build the service with the following commands:
+
----
mvn clean package

sudo docker build -t example/catalog:v2 .
----

. Redeploy the Catalog service version 2 
+
----
oc delete pod -l app=catalog,version=v2
----
+
* Why the delete pod? Based on the Deployment configuration, Kubernetes/OpenShift will recreate the pod, based on the new docker image as it attempts to keep the desired replicas available.

. You can see both versions of the catalog pods running using `oc get pods`:
+
----
oc get pods -l app=catalog -w
----
+
* For the `catalog` service, wait until the Ready column has `2/2` pods and the Status column has `Running`. 

* You should see:
+
----
NAME                          READY     STATUS    RESTARTS   AGE
catalog-v1-6b576ffcf8-g6b48   2/2       Running   0          31m
catalog-v2-7764964564-8h2zt   2/2       Running   0          34s
----

* To exit, press Ctrl+C.


== Pool Ejection
Pool ejection or outlier detection is a resilience strategy that takes place whenever we have a pool of instances/pods to serve a client request. If the request is forwarded to a certain instance and it fails (e.g. returns a 50x error code), then Istio will eject this instance from the pool for a certain sleep window. In our example the sleep window is configured to be 15s. This increases the overall availability by making sure that only healthy pods participate in the pool of instances.

First, you need to insure you have a destinationrule and virtualservice in place to send traffic to the services. 

. Configure the rules to split the traffic 50/50.
+
----
cd ~/lab/rhte-msa-and-service-mesh/

oc create -f istiofiles/destination-rule-catalog-v1-v2.yml -n $OCP_TUTORIAL_PROJECT --as=system:admin
oc create -f istiofiles/virtual-service-catalog-v1_and_v2_50_50.yml -n $OCP_TUTORIAL_PROJECT --as=system:admin
----

. Scale up the number of pods for the `catalog-v2` pod
+
----
oc scale --replicas=2 deployment/catalog-v2
----

. Wait for all of the catalog pods to be in the ready state.
+
----
oc get pods -l app=catalog -w
----

* You should see:
+
----
NAME                          READY     STATUS    RESTARTS   AGE
catalog-v1-6b576ffcf8-g6b48   2/2       Running   0          37m
catalog-v2-7764964564-8h2zt   2/2       Running   0          20m
catalog-v2-7764964564-hrjq5   2/2       Running   0          56s
----

=== Test behavior without failing instances

. Send some requests to the gateway service:
+
----
scripts/run.sh
----

* You should see:
+
----
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 221
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 222
gateway => partner => catalog v2 from '7764964564-8h2zt': 1
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 223
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 224
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 225
gateway => partner => catalog v2 from '7764964564-hrjq5': 1
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 226
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 227
gateway => partner => catalog v2 from '7764964564-8h2zt': 2
----
* You should see the load balancing 50/50 between the two different versions of the catalog service. 
* Within version v2, you will also see that some requests are handled by one pod and some requests are handled by the other pod.

=== Test behavior with failing instance and without pool ejection

. Get the name of the pods for catalog v2
+
----
oc get pods -l app=catalog,version=v2
----

* You should see:
+
----
NAME                          READY     STATUS    RESTARTS   AGE
catalog-v2-7764964564-8h2zt   2/2       Running   0          24m
catalog-v2-7764964564-hrjq5   2/2       Running   0          4m
----

. Now we’ll connect to one the pods and add some erratic behavior on it. 

. Connect to one of your pods using the following command:
+
----
oc exec -it $(oc get pods|grep catalog-v2|awk '{ print $1 }'|head -1) -c catalog /bin/bash
----

* You should see:
+
----
[jboss@catalog-v2-7764964564-8h2zt ~]$
----

* At this point, you are now inside the application container of your pod `catalog-v2-7764964564-8h2zt`. 

. Now execute the following command:
+
----
curl localhost:8080/misbehave
exit
----
* This is a special endpoint that will make our application always return `503` errors.

. Now let's send in 10 requests:
+
----
scripts/run.sh
----

* You should see:
+
----
gateway => partner => catalog v2 from '7764964564-hrjq5': 2
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 228
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 229
gateway => 503 partner => 503 catalog misbehavior from '7764964564-8h2zt'
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 230
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 231
gateway => partner => catalog v2 from '7764964564-hrjq5': 3
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 232
gateway => 503 partner => 503 catalog misbehavior from '7764964564-8h2zt'
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 233
----

* You’ll see that whenever the pod `catalog-v2-7764964564-8h2zt` receives a request, you get a 503 error.

=== Test behavior with failing instance and with pool ejection

. Now let’s add the pool ejection behavior:
+
----
oc replace -f istiofiles/destination-rule-catalog_cb_policy_pool_ejection.yml -n $OCP_TUTORIAL_PROJECT --as=system:admin
----

. Now let's send in 10 requests:
+
----
scripts/run.sh
----

* You should see: 
+
----
gateway => partner => catalog v2 from '7764964564-hrjq5': 4
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 234
gateway => partner => catalog v2 from '7764964564-hrjq5': 5
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 235
gateway => partner => catalog v2 from '7764964564-hrjq5': 6
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 236
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 237
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 238
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 239
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 240
----
* You should see that whenever you get a failing request with 503 from the pod `catalog-v2-7764964564-8h2zt`, it gets ejected from the pool, and it doesn’t receive any more requests until the sleep window expires - which takes at least 15s.

. Wait for 15 seconds and run the test again. You will occasionally see a 503 error, but it will go away after first try ... during the 15 second window.

== Ultimate resilience with retries, circuit breaker, and pool ejection

Even with pool ejection your application doesn’t look that resilient. That’s probably because we’re still letting some errors to be propagated to our clients. But we can improve this. If we have enough instances and/or versions of a specific service running into our system, we can combine multiple Istio capabilities to achieve the ultimate backend resilience: 

* Circuit Breaker to avoid multiple concurrent requests to an instance
* Pool Ejection to remove failing instances from the pool of responding instances
* Retries to forward the request to another instance just in case we get an open circuit breaker and/or pool ejection;

By simply adding a retry configuration to our current virtualservice, we’ll be able to get rid completely of our `503`s requests. This means that whenever we receive a failed request from an ejected instance, Istio will forward the request to another supposably healthy instance.

. Add a retry configuration
+
----
oc replace -f istiofiles/virtual-service-catalog-v1_and_v2_retry.yml -n $OCP_TUTORIAL_PROJECT --as=system:admin
----

. Now let's send in 10 requests:
+
----
scripts/run.sh
----

* You should see:
+
----
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 256
gateway => partner => catalog v2 from '7764964564-hrjq5': 11
gateway => partner => catalog v2 from '7764964564-hrjq5': 12
gateway => partner => catalog v2 from '7764964564-hrjq5': 13
gateway => partner => catalog v2 from '7764964564-hrjq5': 14
gateway => partner => catalog v2 from '7764964564-hrjq5': 15
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 257
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 258
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 259
gateway => partner => catalog v1 from '6b576ffcf8-g6b48': 260
----

* You won’t receive 503`s anymore. But the requests from catalog `v2 are still taking more time to get a response::

NOTE: You may need to wait up to 30 seconds for the retry rule to take effect. Just run the above command again if you see any 503's. You should eventually not see any.

* Our misbehaving pod `catalog-v2-7764964564-8h2zt` never shows up in the console, thanks to pool ejection and retry.

== Clean up

. Scale down the catalog v2 to a single pod
+
----
oc scale deployment catalog-v2 --replicas=1
----

. Remove the route rules before moving on:
+
----
scripts/clean.sh $OCP_TUTORIAL_PROJECT
----

== Congratulations!

In this lab you learned how to implement the pool ejection resilience strategy to increase the overall availability. You also leveraged the circuit breaker pattern to avoid multiple requests to a failed instance.

== References

* https://openshift.com[Red Hat OpenShift]
* https://learn.openshift.com/servicemesh[Learn Istio on OpenShift]
* https://istio.io[Istio Homepage]